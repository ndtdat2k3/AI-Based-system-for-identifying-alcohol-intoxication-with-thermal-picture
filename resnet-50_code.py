# -*- coding: utf-8 -*-
"""drunk_detection(thermal_image)_Resnet_50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ARI2zt1qWneFq0vF_csODhncfNw5AK6P

# 1. Install & Import:
"""

!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.optim.lr_scheduler import ReduceLROnPlateau # Thêm LR Scheduler
from torchvision import transforms, models
import pandas as pd
import cv2
import os
from sklearn.model_selection import train_test_split
from google.colab import drive
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from pathlib import Path
import copy # Thêm để lưu model tốt nhất cho Early Stopping

drive.mount('/content/drive')

"""# 2. Paths & Data Prep (crop bbox từ ảnh gốc, resize 224x224):"""

# Paths 
base_dir = "/content/drive/MyDrive/retrain_thermal_picture/"
image_folder = os.path.join(base_dir, "dataset")
csv_path = os.path.join(base_dir, "annotation", "annotations.csv")
output_dir = os.path.join(base_dir, "resnet_data")  # Folder mới cho cropped images
os.makedirs(output_dir, exist_ok=True)

# Đọc CSV 
df = pd.read_csv(csv_path)
df = df.dropna(subset=['xmin', 'ymin', 'xmax', 'ymax', 'label'])  # Clean data

# --- TẠO DỮ LIỆU RESNET TỪ FILE ANNOTATIONS MỚI NHẤT ---
print("Bắt đầu crop ảnh cho ResNet từ file CSV đã cân bằng...")
cropped_count = 0
for idx, row in df.iterrows():
    img_path = os.path.join(image_folder, row['filename'])
    if not os.path.exists(img_path):
        # print(f"Bỏ qua (không tìm thấy): {row['filename']}")
        continue
    img = cv2.imread(img_path)
    if img is None:
        # print(f"Bỏ qua (lỗi đọc): {row['filename']}")
        continue

    # Crop bbox
    try:
        xmin, ymin, xmax, ymax = int(row['xmin']), int(row['ymin']), int(row['xmax']), int(row['ymax'])
        # Đảm bảo tọa độ hợp lệ
        if xmin >= xmax or ymin >= ymax:
            continue
        cropped = img[ymin:ymax, xmin:xmax]
        if cropped.size == 0:
            continue
    except Exception as e:
        # print(f"Lỗi khi crop {row['filename']}: {e}")
        continue


    # Resize 224x224
    cropped_resized = cv2.resize(cropped, (224, 224))
    bbox_id = idx
    label = row['label']
    save_path = os.path.join(output_dir, f"{Path(row['filename']).stem}_{bbox_id}_{label}.jpg")
    cv2.imwrite(save_path, cropped_resized)
    cropped_count += 1

print(f"Đã crop và lưu {cropped_count} ảnh vào {output_dir}")

"""3. Custom Dataset cho PyTorch:"""

class DrunkDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.images = []
        self.labels = []
        label_map = {'Drunk': 0, 'Sober': 1}  # Binary: 0=Drunk, 1=Sober
        for file in os.listdir(image_dir):
            if file.endswith('.jpg'):
                label_str = file.split('_')[-1].split('.')[0]  # Parse label từ tên
                if label_str in label_map: # Chỉ thêm nếu là Drunk hoặc Sober
                    self.images.append(file)
                    self.labels.append(label_map.get(label_str))

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.images[idx])
        image = Image.open(img_path).convert('RGB')
        label = self.labels[idx]
        if self.transform:
            image = self.transform(image)
        return image, label

# === TĂNG CƯỜNG AUGMENTATION ===
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(degrees=15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
transform_val = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Tạo dataset và split (thêm stratify)
# Chạy lại DrunkDataset để nó đọc từ folder 'resnet_data' mới nhất
full_dataset = DrunkDataset(output_dir, transform=transform_train)
label_list = [full_dataset.labels[i] for i in range(len(full_dataset))]  # Lấy labels cho stratify

# Đếm số lượng
print(f"Tổng số ảnh cropped: {len(full_dataset)}")
print(f"  Số ảnh Drunk (0): {label_list.count(0)}")
print(f"  Số ảnh Sober (1): {label_list.count(1)}")

train_idx, val_idx = train_test_split(range(len(full_dataset)), test_size=0.2, random_state=42, stratify=label_list)

train_dataset = torch.utils.data.Subset(full_dataset, train_idx)
# Val dùng transform_val (không augmentation)
val_dataset_no_aug = DrunkDataset(output_dir, transform=transform_val)
val_dataset = torch.utils.data.Subset(val_dataset_no_aug, val_idx)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# Thêm: Visualize 6 sample cropped images (với denormalize để tránh warning và chói)
fig, axes = plt.subplots(2, 3, figsize=(10, 8))
mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)  # Mean ImageNet
std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)    # Std ImageNet

for i in range(6):
    sample_img, sample_label = full_dataset[i]
    # Denormalize: (img * std) + mean, rồi clamp về [0,1] để chuẩn cho imshow
    denorm_img = torch.clamp(sample_img * std + mean, 0, 1)
    ax = axes[i//3, i%3]
    ax.imshow(denorm_img.permute(1, 2, 0).numpy())  # Bây giờ range [0,1], không warning
    ax.set_title(f"Label: {'Drunk' if sample_label == 0 else 'Sober'}")
    ax.axis('off')
plt.tight_layout()
plt.show()

"""4. Train ResNet-50:"""

from torchvision.models import ResNet50_Weights

# === LOAD MODEL KHÔNG PRE-TRAIN (TRAIN TỪ ĐẦU) ===
model = models.resnet50(weights=None) # weights=None
num_features = model.fc.in_features

# === (THAY ĐỔI) THÊM DROPOUT VÀO LỚP FC ===
model.fc = nn.Sequential(
    nn.Dropout(p=0.5),
    nn.Linear(num_features, 2) # 2 classes
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

criterion = nn.CrossEntropyLoss()

# --- GIAI ĐOẠN 1: TRAINING (TỪ ĐẦU) ---
print("\n--- BẮT ĐẦU TRAINING (TỪ ĐẦU - FROM SCRATCH) ---")
NUM_EPOCHS = 100 # Tổng epoch = 100
LEARNING_RATE = 1e-3 # LR cao vì train từ đầu
PATIENCE_EARLY_STOP = 20 # Tăng patience
PATIENCE_SCHEDULER = 5 # Tăng patience

# Mở băng toàn bộ model (mặc định khi train từ đầu)
for param in model.parameters():
    param.requires_grad = True

# Optimizer cho tất cả params, với LR 1e-3
# === THÊM WEIGHT_DECAY (L2 REGULARIZATION) ĐỂ CHỐNG OVERFITTING ===
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)

# Thêm LR Scheduler 
scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=PATIENCE_SCHEDULER)

# Thêm Early Stopping
best_val_loss = float('inf')
epochs_no_improve = 0
best_model_wts = copy.deepcopy(model.state_dict()) # Lưu trọng số model tốt nhất

# Lịch sử training
train_losses, val_losses = [], []
train_accs, val_accs = [], []

total_epochs = NUM_EPOCHS

for epoch in range(total_epochs):
    actual_epoch = epoch
    # Train
    model.train()
    running_loss = 0.0
    correct_train = 0
    total_train = 0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    train_loss = running_loss / len(train_loader)
    train_acc = 100 * correct_train / total_train
    train_losses.append(train_loss)
    train_accs.append(train_acc)

    # Val
    model.eval()
    val_loss = 0.0
    correct_val = 0
    total_val = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()

    val_loss = val_loss / len(val_loader)
    val_acc = 100 * correct_val / total_val
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Epoch {actual_epoch+1}/{total_epochs} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%")

    # Cập nhật LR Scheduler
    scheduler.step(val_loss)

    # Logic Early Stopping
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        epochs_no_improve = 0
        best_model_wts = copy.deepcopy(model.state_dict())
        print(f"  -> Val loss cải thiện, lưu model tốt nhất (Val Loss: {best_val_loss:.4f})")
    else:
        epochs_no_improve += 1
        print(f"  -> Val loss không cải thiện. Đã {epochs_no_improve}/{PATIENCE_EARLY_STOP} epochs.")

    if epochs_no_improve >= PATIENCE_EARLY_STOP:
        print(f"EARLY STOPPING! Dừng ở epoch {actual_epoch+1}.")
        break

# Load lại model tốt nhất đã lưu
print("Hoàn tất training. Load lại trọng số tốt nhất (best_val_loss).")
model.load_state_dict(best_model_wts)


# Plot curves
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(train_accs, label='Train Acc')
plt.plot(val_accs, label='Val Acc')
plt.title('Model Accuracy (Train from Scratch)')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend()
plt.grid()

plt.subplot(1, 2, 2)
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.title('Model Loss (Train from Scratch)')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

# Save model
torch.save(model.state_dict(), os.path.join(base_dir, "resnet50_classifier_v3_from_scratch.pth"))
print("Model v3 (from_scratch) đã lưu!")

"""5. Evaluation the Model


"""

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# Eval trên val set (tương tự train loop)
model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for inputs, labels in val_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        all_preds.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Confusion matrix
cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Drunk', 'Sober'], yticklabels=['Drunk', 'Sober'])
plt.title('Confusion Matrix (Val Set)')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

print(classification_report(all_labels, all_preds, target_names=['Drunk', 'Sober']))

"""6. Testing"""

# Thêm định nghĩa class_names (0: Drunk, 1: Sober)
class_names = ['Drunk', 'Sober']

# Tạo test dataset (dùng val_loader như test cho đơn giản; nếu có test riêng, tạo tương tự bước 3)
test_loader = val_loader  # Hoặc tạo Subset mới cho test nếu split 3-way

# Hàm predict đơn lẻ (giữ nguyên)
def predict_single(model, image_path, transform, class_names, device=device):
    image = Image.open(image_path).convert('RGB')
    image_tensor = transform(image).unsqueeze(0).to(device)
    model.eval()
    with torch.no_grad():
        output = model(image_tensor)
        pred_class = torch.argmax(output, 1).item()
        confidence = torch.softmax(output, 1).max().item()
    return class_names[pred_class], confidence

# Test trên 5 sample test images (chọn random từ test set hoặc folder)
test_samples = []  # Lấy từ test_loader hoặc list files
for i, (images, true_labels) in enumerate(test_loader):
    if i >= 1:  # Lấy 1 batch đầu
        for j in range(min(5, len(images))):  # 5 samples
            # Denormalize để save temp image đúng (fix chói như bước 3)
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            denorm_img = (images[j].cpu().permute(1, 2, 0).numpy() * std + mean)
            denorm_img = np.clip(denorm_img, 0, 1) * 255  # Scale về [0,255]
            denorm_img = denorm_img.astype(np.uint8)

            temp_path = f"/tmp/test_sample_{j}.jpg"
            Image.fromarray(denorm_img).save(temp_path)
            pred_label, conf = predict_single(model, temp_path, transform_val, class_names)
            true_label = class_names[true_labels[j].item()]  # Giờ class_names defined
            correct = "✅" if pred_label == true_label else "❌"
            print(f"Sample {j+1}: True: {true_label} | Pred: {pred_label} (conf: {conf:.2f}) | {correct}")

            # Visualize
            plt.figure(figsize=(6, 6))
            plt.imshow(denorm_img)
            plt.title(f"True: {true_label} | Pred: {pred_label} ({conf:.2f})")
            plt.axis('off')
            plt.show()
        break

# Batch test accuracy 
model.eval()
correct_test = 0
total_test = 0
all_test_preds, all_test_labels = [], []
with torch.no_grad():
    for inputs, labels in test_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        total_test += labels.size(0)
        correct_test += (predicted == labels).sum().item()
        all_test_preds.extend(predicted.cpu().numpy())
        all_test_labels.extend(labels.cpu().numpy())

print(f"Test Accuracy: {100 * correct_test / total_test:.2f}%")
from sklearn.metrics import classification_report
print(classification_report(all_test_labels, all_test_preds, target_names=class_names))
